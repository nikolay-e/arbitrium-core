#!/usr/bin/env python3
"""
Certamen Framework - Benchmark Comparison Example

Compare single-model vs tournament performance on your own questions.
Perfect for: Evaluating whether tournament is worth the cost.

Expected runtime: ~10-20 minutes
Expected cost: ~$1-3 (compares multiple approaches)
"""

import asyncio
import time

from certamen_core import Certamen


async def main():
    """Run a micro-benchmark: Single model vs Tournament."""

    certamen = await Certamen.from_config("config.example.yml")

    print("=" * 80)
    print("‚öñÔ∏è  SINGLE MODEL vs TOURNAMENT BENCHMARK")
    print("=" * 80)

    # Test question
    question = """
    Evaluate whether we should build our MVP with React or Vue.js.

    Context:
    - Small team (2 developers)
    - Timeline: 3 months to MVP
    - Need to hire more devs later
    - Product: B2B SaaS dashboard
    """

    print(f"\n‚ùì Test Question:\n{question}\n")

    # ------------------------------------------------------------------
    # APPROACH 1: Single Model (fastest, cheapest)
    # ------------------------------------------------------------------
    # Use first available model
    first_model_key = next(iter(certamen.healthy_models.keys()))
    first_model = certamen.healthy_models[first_model_key]

    print("=" * 80)
    print(f"ü§ñ APPROACH 1: Single Model ({first_model.display_name})")
    print("=" * 80)

    start = time.time()
    single_response = await certamen.run_single_model(
        first_model_key, question
    )
    single_time = time.time() - start

    print(f"\n‚úÖ Completed in {single_time:.1f}s")
    print(f"üí∞ Cost: ${single_response.cost:.4f}")
    print("\nüìù Response Preview:")
    print(single_response.content[:300] + "...")

    # ------------------------------------------------------------------
    # APPROACH 2: All Models (no tournament, just comparison)
    # ------------------------------------------------------------------
    print("\n" + "=" * 80)
    print("üîÑ APPROACH 2: All Models (No Tournament)")
    print("=" * 80)

    start = time.time()
    all_responses = await certamen.run_all_models(question)
    all_time = time.time() - start

    all_cost = sum(r.cost for r in all_responses.values() if not r.is_error())

    print(f"\n‚úÖ Completed in {all_time:.1f}s")
    print(f"üí∞ Total Cost: ${all_cost:.4f}")
    print("\nüìä Individual Costs:")
    for model_key, response in all_responses.items():
        if not response.is_error():
            print(f"   {model_key}: ${response.cost:.4f}")

    # ------------------------------------------------------------------
    # APPROACH 3: Tournament (synthesis + elimination)
    # ------------------------------------------------------------------
    print("\n" + "=" * 80)
    print("üèÜ APPROACH 3: Tournament")
    print("=" * 80)

    start = time.time()
    _tournament_result, tournament_metrics = await certamen.run_tournament(
        question
    )
    tournament_time = time.time() - start

    print(f"\n‚úÖ Completed in {tournament_time:.1f}s")
    print(f"üí∞ Total Cost: ${tournament_metrics['total_cost']:.4f}")
    print(f"üèÜ Champion: {tournament_metrics['champion_model']}")
    print(
        f"üóëÔ∏è  Eliminated: {len(tournament_metrics['eliminated_models'])} models"
    )

    # ------------------------------------------------------------------
    # COMPARISON TABLE
    # ------------------------------------------------------------------
    print("\n" + "=" * 80)
    print("üìä COMPARISON SUMMARY")
    print("=" * 80)

    print(f"\n{'Approach':<30} {'Time':>10} {'Cost':>10} {'Models':>10}")
    print("-" * 80)
    print(
        f"{'1. Single Model (' + first_model.display_name + ')':<30} {single_time:>9.1f}s ${single_response.cost:>8.4f} {1:>10}"
    )
    print(
        f"{'2. All Models (Independent)':<30} {all_time:>9.1f}s ${all_cost:>8.4f} {len(all_responses):>10}"
    )
    print(
        f"{'3. Tournament (Synthesis)':<30} {tournament_time:>9.1f}s ${tournament_metrics['total_cost']:>8.4f} {certamen.healthy_model_count:>10}"
    )

    # Cost multipliers
    tournament_cost_mult = (
        tournament_metrics["total_cost"] / single_response.cost
    )
    tournament_time_mult = tournament_time / single_time

    print(f"\nüí° Tournament Cost Multiple: {tournament_cost_mult:.1f}x")
    print(f"‚è±Ô∏è  Tournament Time Multiple: {tournament_time_mult:.1f}x")

    # ------------------------------------------------------------------
    # WHEN TO USE EACH
    # ------------------------------------------------------------------
    print("\n" + "=" * 80)
    print("üéØ RECOMMENDATIONS")
    print("=" * 80)

    print(
        f"""
    Use Single Model when:
    ‚úÖ Budget < $0.20 per query
    ‚úÖ Time sensitive (< 1 minute)
    ‚úÖ Exploratory questions
    ‚úÖ Reversible decisions

    Use All Models when:
    ‚úÖ Need multiple perspectives
    ‚úÖ But don't need synthesis
    ‚úÖ Will manually review responses

    Use Tournament when:
    ‚úÖ Decision value > $1,000 ({1000/tournament_metrics['total_cost']:.0f}x cost)
    ‚úÖ Irreversible decision
    ‚úÖ Stakeholder buy-in needed (synthesis helps)
    ‚úÖ Worth extra {tournament_time-single_time:.0f} seconds for quality
    """
    )


if __name__ == "__main__":
    asyncio.run(main())
